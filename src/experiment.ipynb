{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import hour, dayofweek, month, col\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"NYC_Taxi_Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# For convenience, I save the data in the NYC folder which is in the same directory as this notebook\n",
    "parquet_file_path = os.path.join(cwd, \"NYC\", \"*.parquet\")\n",
    "taxi_df = spark.read.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_analysis(taxi_df = taxi_df):\n",
    "    # cwd = os.getcwd()\n",
    "\n",
    "    # # For convenience, I save the data in the NYC folder which is in the same directory as this notebook\n",
    "    # parquet_file_path = os.path.join(cwd, \"NYC\", \"*.parquet\")\n",
    "    # taxi_df = spark.read.parquet(parquet_file_path)\n",
    "    \n",
    "    # Calculate Duration and Distance:\n",
    "    # Create a new column for trip duration and calculate it using the difference between pickup and dropoff times. Also,calculate the average distance for each record:\n",
    "    taxi_df = taxi_df.withColumn(\"trip_duration\", F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\"))\n",
    "    taxi_df = taxi_df.withColumn(\"avg_distance\", (F.col(\"trip_distance\") / F.col(\"passenger_count\")))\n",
    "    \n",
    "    # Extract Time of Day, Day of Week, and Month of Year:\n",
    "    taxi_df = taxi_df.withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "    taxi_df = taxi_df.withColumn(\"pickup_day_of_week\", F.dayofweek(\"tpep_pickup_datetime\"))\n",
    "    taxi_df = taxi_df.withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\"))\n",
    "    \n",
    "    # Group the data by time of day, day of week, and month of year, and calculate the average duration and distance for each group:\n",
    "    agg_df = taxi_df.groupBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\").agg(F.avg(\"trip_duration\").alias(\"avg_duration\"),F.avg(\"avg_distance\").alias(\"avg_distance\")).orderBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\")\n",
    "    \n",
    "    # Group and Count Pickup Locations:\n",
    "    pickup_locations = taxi_df.groupBy(\"PULocationID\").count().orderBy(F.desc(\"count\"))\n",
    "    top_pickup_locations = pickup_locations.limit(10)\n",
    "    dropoff_locations = taxi_df.groupBy(\"DOLocationID\").count().orderBy(F.desc(\"count\"))\n",
    "    top_dropoff_locations = dropoff_locations.limit(10)\n",
    "    \n",
    "    agg_df.show()\n",
    "    print(\"Top 10 Pickup Locations:\")\n",
    "    top_pickup_locations.show()\n",
    "    print(\"Top 10 Dropoff Locations:\")\n",
    "    top_dropoff_locations.show()\n",
    "    return taxi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|pickup_hour|pickup_day_of_week|pickup_month|      avg_duration|      avg_distance|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|          0|                 1|           1| 888.9417608770127|3.2521391261294736|\n",
      "|          0|                 1|           2| 726.9726522187823|  2.89541996285979|\n",
      "|          0|                 1|           3| 888.4640534063677|2.8471578638497625|\n",
      "|          0|                 1|           4| 854.6067429406037|2.9782131248083417|\n",
      "|          0|                 1|           5| 883.9497659700705|2.8323654661521616|\n",
      "|          0|                 1|           6| 954.6639178045153| 2.721734596752614|\n",
      "|          0|                 1|           7|1011.6342042755344| 2.835302943232113|\n",
      "|          0|                 1|           8| 997.7019489609131|2.8669667749237213|\n",
      "|          0|                 1|           9|1057.5752172184677|2.5259332031492243|\n",
      "|          0|                 1|          10| 983.6769738118331|2.4983152044322683|\n",
      "|          0|                 1|          11| 935.9346515215258| 2.539192359159652|\n",
      "|          0|                 1|          12| 972.2170118615943| 3.150238748796069|\n",
      "|          0|                 2|           1| 915.7601380500431| 4.946866130558188|\n",
      "|          0|                 2|           2|1105.8451242829829| 4.494771421675113|\n",
      "|          0|                 2|           3|  820.387349953832| 5.216901033850259|\n",
      "|          0|                 2|           4| 960.7860179499291| 5.048313027017737|\n",
      "|          0|                 2|           5| 951.8014098690836| 4.508350586080599|\n",
      "|          0|                 2|           6| 971.4392452830189| 4.867711624592842|\n",
      "|          0|                 2|           7| 990.5213815789474|4.3910269512339335|\n",
      "|          0|                 2|           8|1029.8973904639174| 5.536793713681965|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Top 10 Pickup Locations:\n",
      "+------------+-------+\n",
      "|PULocationID|  count|\n",
      "+------------+-------+\n",
      "|         237|1553554|\n",
      "|         236|1424614|\n",
      "|         161|1091329|\n",
      "|         132|1025063|\n",
      "|         186|1019650|\n",
      "|         142| 989927|\n",
      "|         170| 967766|\n",
      "|         162| 954917|\n",
      "|         239| 932473|\n",
      "|         141| 909845|\n",
      "+------------+-------+\n",
      "\n",
      "Top 10 Dropoff Locations:\n",
      "+------------+-------+\n",
      "|DOLocationID|  count|\n",
      "+------------+-------+\n",
      "|         236|1434919|\n",
      "|         237|1356518|\n",
      "|         161|1001077|\n",
      "|         170| 920433|\n",
      "|         141| 902052|\n",
      "|         239| 886837|\n",
      "|         142| 854324|\n",
      "|          48| 782803|\n",
      "|         238| 779046|\n",
      "|         162| 772823|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_df = trip_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tip analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tip_analysis(taxi_df=taxi_df):\n",
    "    \n",
    "    # Calculate the tip percentage for each trip. This involves dividing the tip amount by the total fare amount\n",
    "    tip_analysis_df = taxi_df.withColumn(\"tip_percentage\", F.col(\"tip_amount\") / F.col(\"total_amount\") * 100)\n",
    "    tip_analysis_df = tip_analysis_df.withColumn(\"tip_percentage\", F.when(F.col(\"tip_percentage\") <= 100, F.col(\"tip_percentage\")).otherwise(0))  # Handle possible outliers\n",
    "\n",
    "    # Group by pickup and dropoff locations and calculate average tip percentage and average distance\n",
    "    tip_location_analysis = tip_analysis_df.groupBy(\"PULocationID\", \"DOLocationID\").agg(F.avg(\"tip_percentage\").alias(\"avg_tip_percentage\"), F.avg(\"trip_distance\").alias(\"avg_distance\")).orderBy(F.desc(\"avg_tip_percentage\"))\n",
    "    # Calculate the average tip percentage and total tip amount for each time interval\n",
    "    tip_time_analysis = tip_analysis_df.groupBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\").agg(F.avg(\"tip_percentage\").alias(\"avg_tip_percentage\"), F.sum(\"tip_amount\").alias(\"total_tip_amount\")).orderBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\")\n",
    "\n",
    "    # Group the data by payment type and calculate statistics such as the average tip percentage, average tip amount, and total tip amount for each payment type\n",
    "    payment_tip_analysis = tip_analysis_df.groupBy(\"payment_type\").agg(F.avg(\"tip_percentage\").alias(\"avg_tip_percentage\"), F.avg(\"tip_amount\").alias(\"avg_tip_amount\"), F.sum(\"tip_amount\").alias(\"total_tip_amount\"))\n",
    "    \n",
    "    print(\"Tip Analysis by Location:\")\n",
    "    tip_location_analysis.show()\n",
    "    print(\"Tip Analysis by Time:\")\n",
    "    tip_time_analysis.show()\n",
    "    print(\"Payment and Tip Analysis:\")\n",
    "    payment_tip_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tip Analysis by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+-----------------+\n",
      "|PULocationID|DOLocationID|avg_tip_percentage|     avg_distance|\n",
      "+------------+------------+------------------+-----------------+\n",
      "|         187|         251|56.179775280898866|             1.54|\n",
      "|         176|         176|          53.90625|             0.32|\n",
      "|          96|         236| 48.85197850512946|            11.31|\n",
      "|         109|         172|46.948356807511736|             2.09|\n",
      "|         251|         161|46.200737170399776|            19.58|\n",
      "|         120|         151| 43.01075268817204|             0.73|\n",
      "|         118|         214|  41.3564929693962|             4.16|\n",
      "|         172|         214| 39.96670910603205|3.293333333333333|\n",
      "|         208|         114| 38.55192080359299|            8.975|\n",
      "|          34|           1| 38.41764929631039|            15.11|\n",
      "|          82|         253| 38.28483920367534|             2.48|\n",
      "|         112|         214| 37.67972235994051|            16.03|\n",
      "|          96|         177|37.522401433691755|4.699999999999999|\n",
      "|          98|         253|36.231884057971016|              5.0|\n",
      "|          34|         236|36.050057963748074|            8.965|\n",
      "|         214|         172| 35.85657881901123|4.154999999999999|\n",
      "|         118|         172| 34.72222222222222|             3.47|\n",
      "|         175|          28|34.692603978968656|4.966666666666666|\n",
      "|          98|         191| 34.63261668452944|            2.286|\n",
      "|         202|         191| 34.56221198156682|            16.99|\n",
      "+------------+------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Tip Analysis by Time:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|pickup_hour|pickup_day_of_week|pickup_month|avg_tip_percentage|  total_tip_amount|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|          0|                 1|           1|10.509465545793379| 6285.209999999994|\n",
      "|          0|                 1|           2|10.960206819588734|  8032.64000000002|\n",
      "|          0|                 1|           3|11.105551868685199|12657.859999999993|\n",
      "|          0|                 1|           4|11.314341113424645|18623.729999999974|\n",
      "|          0|                 1|           5|12.117369325323516|36288.720000000074|\n",
      "|          0|                 1|           6| 12.19923946046261| 36665.33000000003|\n",
      "|          0|                 1|           7|11.893981566402744| 34842.08999999987|\n",
      "|          0|                 1|           8|12.147081300758224|47726.230000000294|\n",
      "|          0|                 1|           9|12.178405026405924|43128.000000000204|\n",
      "|          0|                 1|          10|12.615973213402029|64931.850000001265|\n",
      "|          0|                 1|          11|12.530480004739998|52851.220000000736|\n",
      "|          0|                 1|          12|12.381016811103633| 40848.40999999994|\n",
      "|          0|                 2|           1| 9.083634613534988| 2937.700000000002|\n",
      "|          0|                 2|           2| 9.046358567573382| 3625.010000000003|\n",
      "|          0|                 2|           3| 8.912576772587578| 5795.150000000005|\n",
      "|          0|                 2|           4| 9.551659729865403| 6010.509999999994|\n",
      "|          0|                 2|           5|10.373283655908706|          13920.53|\n",
      "|          0|                 2|           6| 10.56761617803041| 16237.30999999995|\n",
      "|          0|                 2|           7|10.498185701824484|17556.769999999986|\n",
      "|          0|                 2|           8|10.404594988408133| 20802.27999999999|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Payment and Tip Analysis:\n",
      "+------------+--------------------+--------------------+-------------------+\n",
      "|payment_type|  avg_tip_percentage|      avg_tip_amount|   total_tip_amount|\n",
      "+------------+--------------------+--------------------+-------------------+\n",
      "|           0|   8.070807011379319|  2.1700068168216093| 3208778.2300000293|\n",
      "|           5|                 0.0|                 0.0|                0.0|\n",
      "|           1|  15.331362787552713|  3.0755510306721945|6.913956203000465E7|\n",
      "|           3|   0.145542231109957|-0.01167058060330...|-1799.0200000000004|\n",
      "|           2|-0.00155152365292...|4.108590704647675E-4| 2740.4299999999994|\n",
      "|           4| -0.3495420614922692|0.022958282745690756|            2779.72|\n",
      "+------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tip_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fare Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fare by Pickup & Drop Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+\n",
      "|PULocationID|DOLocationID|          avg_fare|\n",
      "+------------+------------+------------------+\n",
      "|         154|          28|            1164.0|\n",
      "|         234|         189| 843.4665424430641|\n",
      "|           1|         247|             420.0|\n",
      "|          83|         136|             378.5|\n",
      "|           5|          74|             306.0|\n",
      "|          54|         265|             275.5|\n",
      "|          29|         264|213.75227272727273|\n",
      "|           2|         265|            200.25|\n",
      "|           6|         265|            192.25|\n",
      "|         123|         265|            177.35|\n",
      "|         235|         115|             170.0|\n",
      "|         221|         265|             160.0|\n",
      "|         253|         208|             160.0|\n",
      "|         112|         109|             155.0|\n",
      "|         204|         265|           152.375|\n",
      "|          44|         138|             151.5|\n",
      "|         118|         265|            151.25|\n",
      "|          55|           1|             150.0|\n",
      "|          10|           1|148.21428571428572|\n",
      "|         221|           1|             148.0|\n",
      "+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Average Fare by Passenger Count:\n",
      "+---------------+------------------+\n",
      "|passenger_count|          avg_fare|\n",
      "+---------------+------------------+\n",
      "|           null| 25.50271663190539|\n",
      "|            0.0|12.251760226150822|\n",
      "|            1.0|12.709557736571188|\n",
      "|            2.0| 13.77639929394148|\n",
      "|            3.0|13.555663818461737|\n",
      "|            4.0|14.284687986716264|\n",
      "|            5.0|12.666400646383593|\n",
      "|            6.0| 12.75109443706296|\n",
      "|            7.0| 52.91679487179488|\n",
      "|            8.0| 49.14408163265307|\n",
      "|            9.0|             61.35|\n",
      "|           96.0|              11.5|\n",
      "|          112.0|               9.0|\n",
      "+---------------+------------------+\n",
      "\n",
      "Correlation between Fare Amount and Trip Distance:\n",
      "Correlation coefficient: 0.0008730862657094112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def fare_analysis(taxi_df = taxi_df):\n",
    "    # Calculate the average fare by pickup and dropoff location\n",
    "    fare_location_analysis = taxi_df.groupBy(\"PULocationID\", \"DOLocationID\").agg(F.avg(\"fare_amount\").alias(\"avg_fare\")).orderBy(F.desc(\"avg_fare\"))\n",
    "    # Calculate the average fare for different passenger counts to analyze the correlation between passenger count and fare amount\n",
    "    fare_passenger_analysis = taxi_df.groupBy(\"passenger_count\").agg(F.avg(\"fare_amount\").alias(\"avg_fare\")).orderBy(\"passenger_count\")\n",
    "    # Calculate the Pearson correlation coefficient to investigate the correlation between fare amount and trip distance\n",
    "    fare_distance_correlation = taxi_df.select(F.corr(\"fare_amount\", \"trip_distance\").alias(\"correlation\")).collect()[0][\"correlation\"]\n",
    "    \n",
    "    print(\"Average Fare by Pickup & Drop Location:\")\n",
    "    fare_location_analysis.show()\n",
    "    \n",
    "    print(\"Average Fare by Passenger Count:\")\n",
    "    fare_passenger_analysis.show()\n",
    "    \n",
    "    print(\"Correlation between Fare Amount and Trip Distance:\")\n",
    "    print(\"Correlation coefficient:\", fare_distance_correlation)\n",
    "\n",
    "fare_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/05 11:41:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[739.837s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 89.0 (TID 356): Retried waiting for GCLocker too often allocating 1048578 words\n",
      "[739.850s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 89.0 (TID 356): Retried waiting for GCLocker too often allocating 1048578 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 11:41:11 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/08/05 11:41:12 WARN TaskMemoryManager: Failed to allocate a page (8388608 bytes), try again.\n",
      "23/08/05 11:41:12 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 91:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------------+------------+-----------------+\n",
      "|PULocationID|DOLocationID|pickup_hour|pickup_day_of_week|pickup_month|   avg_trip_speed|\n",
      "+------------+------------+-----------+------------------+------------+-----------------+\n",
      "|           1|           1|          0|                 1|           4|              0.0|\n",
      "|           1|           1|          0|                 2|           3|              0.0|\n",
      "|           1|           1|          0|                 2|          11|              0.0|\n",
      "|           1|           1|          0|                 4|           6|              0.0|\n",
      "|           1|           1|          0|                 7|           5|2438.181818181818|\n",
      "|           1|           1|          1|                 1|           6|              0.0|\n",
      "|           1|           1|          1|                 1|          11|              0.0|\n",
      "|           1|           1|          1|                 2|           3|              0.0|\n",
      "|           1|           1|          1|                 2|          10|              0.0|\n",
      "|           1|           1|          1|                 3|           4|              0.0|\n",
      "|           1|           1|          1|                 3|          10|              0.0|\n",
      "|           1|           1|          1|                 4|           6|              0.0|\n",
      "|           1|           1|          1|                 4|          12|              0.0|\n",
      "|           1|           1|          1|                 5|           9|              4.8|\n",
      "|           1|           1|          1|                 7|           4|              0.0|\n",
      "|           1|           1|          2|                 2|           3|              0.0|\n",
      "|           1|           1|          2|                 2|           8|              0.0|\n",
      "|           1|           1|          2|                 2|           9|              0.0|\n",
      "|           1|           1|          2|                 3|           9|              0.0|\n",
      "|           1|           1|          2|                 3|          11|              0.0|\n",
      "+------------+------------+-----------+------------------+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def trafic_analysis(taxi_df = taxi_df):\n",
    "    \n",
    "    # Calculate the speed per hour\n",
    "    trip_speed_df = taxi_df.withColumn(\"trip_speed\", (F.col(\"trip_distance\") / (F.col(\"trip_duration\") / 3600)))\n",
    "    # Group by Trip Time Intervals:\n",
    "    trip_time_speed_analysis = trip_speed_df.groupBy(\"PULocationID\", \"DOLocationID\", \"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\").agg(F.avg(\"trip_speed\").alias(\"avg_trip_speed\")).orderBy(\"PULocationID\", \"DOLocationID\", \"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\")\n",
    "    \n",
    "    trip_time_speed_analysis.show()\n",
    "    \n",
    "\n",
    "trafic_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 11:44:57 WARN Instrumentation: [ece564a5] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/08/05 11:44:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/05 11:44:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/08/05 11:44:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def demand_analysis(taxi_df = taxi_df):\n",
    "    \n",
    "    # Create features from the date and time of pickups\n",
    "    demand_df = taxi_df.withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "    demand_df = demand_df.withColumn(\"pickup_day_of_week\", F.dayofweek(\"tpep_pickup_datetime\"))\n",
    "    demand_df = demand_df.withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\"))\n",
    "\n",
    "    # Regression Model (Linear Regression):\n",
    "    feature_columns = [\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "    demand_df = assembler.transform(demand_df)\n",
    "    \n",
    "    feature_columns = [\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"input_features\")\n",
    "    demand_df = assembler.transform(demand_df)\n",
    "    \n",
    "    # Aggregate data for regression\n",
    "    regression_df = demand_df.groupBy(\"pickup_hour\").agg(F.sum(\"passenger_count\").alias(\"total_pickups\"),F.first(\"input_features\").alias(\"features\"))\n",
    "    \n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_pickups\")\n",
    "    lr_model = lr.fit(regression_df)\n",
    "    \n",
    "    return lr_model\n",
    "\n",
    "lr_model = demand_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Pickups for Next Hour: 1341885.7375189392\n"
     ]
    }
   ],
   "source": [
    "def prediction_hour(lr_model = lr_model, taxi_df = taxi_df):\n",
    "    \n",
    "    # Get the last record in the Parquet data\n",
    "    last_record = taxi_df.orderBy(F.desc(\"tpep_pickup_datetime\")).limit(1).collect()[0]\n",
    "\n",
    "    # Extract relevant information from the last record\n",
    "    last_pickup_datetime = last_record[\"tpep_pickup_datetime\"]\n",
    "    next_pickup_datetime = last_pickup_datetime + timedelta(hours=1)\n",
    "\n",
    "    # Extract features for the next hour\n",
    "    next_hour = next_pickup_datetime.hour\n",
    "    next_day_of_week = next_pickup_datetime.weekday()\n",
    "    next_month = next_pickup_datetime.month\n",
    "\n",
    "    # Create a DataFrame for the next hour\n",
    "    next_pickup_row_hour = Row(pickup_hour=next_hour, pickup_day_of_week=next_day_of_week, pickup_month=next_month)\n",
    "    next_pickup_df_hour = spark.createDataFrame([next_pickup_row_hour])\n",
    "\n",
    "    # Convert columns to feature vector using VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=[\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"], outputCol=\"features\")\n",
    "\n",
    "    # Transform the features and make predictions for the next hour\n",
    "    next_pickup_df_hour = assembler.transform(next_pickup_df_hour)\n",
    "    predictions_hour = lr_model.transform(next_pickup_df_hour)\n",
    "    predicted_pickups_hour = predictions_hour.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "\n",
    "    print(\"Predicted Pickups for Next Hour:\", predicted_pickups_hour)\n",
    "\n",
    "prediction_hour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Pickups for Next Day: 1461753.3968590284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def prediction_day(lr_model=lr_model, taxi_df=taxi_df):\n",
    "    \n",
    "    # Get the last record in the Parquet data\n",
    "    last_record = taxi_df.orderBy(F.desc(\"tpep_pickup_datetime\")).limit(1).collect()[0]\n",
    "\n",
    "    # Extract relevant information from the last record\n",
    "    last_pickup_datetime = last_record[\"tpep_pickup_datetime\"]\n",
    "    next_pickup_datetime = last_pickup_datetime + timedelta(hours=1)\n",
    "    \n",
    "    # Calculate the next day and month\n",
    "    next_day_datetime = last_pickup_datetime + timedelta(days=1)\n",
    "    next_day_hour = next_day_datetime.hour\n",
    "    next_day_of_week = next_day_datetime.weekday()\n",
    "    next_month = next_day_datetime.month\n",
    "\n",
    "    # Create a DataFrame for the next day\n",
    "    next_pickup_row_day = Row(pickup_hour=next_day_hour, pickup_day_of_week=next_day_of_week, pickup_month=next_month)\n",
    "    next_pickup_df_day = spark.createDataFrame([next_pickup_row_day])\n",
    "\n",
    "    # Convert columns to feature vector using VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=[\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"], outputCol=\"features\")\n",
    "\n",
    "    # Convert columns to feature vector using VectorAssembler\n",
    "    next_pickup_df_day = assembler.transform(next_pickup_df_day)\n",
    "\n",
    "    # Make predictions for the next day\n",
    "    predictions_day = lr_model.transform(next_pickup_df_day)\n",
    "    predicted_pickups_day = predictions_day.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "\n",
    "    print(\"Predicted Pickups for Next Day:\", predicted_pickups_day)\n",
    "    \n",
    "prediction_day()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:==================================================>    (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Pickups for Next Month: 1198863.0639412608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def prediction_month(lr_model=lr_model, taxi_df=taxi_df):\n",
    "        \n",
    "    # Get the last record in the Parquet data\n",
    "    last_record = taxi_df.orderBy(F.desc(\"tpep_pickup_datetime\")).limit(1).collect()[0]\n",
    "\n",
    "    # Extract relevant information from the last record\n",
    "    last_pickup_datetime = last_record[\"tpep_pickup_datetime\"]\n",
    "    next_pickup_datetime = last_pickup_datetime + timedelta(hours=1)\n",
    "    \n",
    "    # Calculate the next month\n",
    "    next_month_datetime = last_pickup_datetime + timedelta(days=30)  # Assuming 30 days in a month\n",
    "    next_month_hour = next_month_datetime.hour\n",
    "    next_day_of_week = next_month_datetime.weekday()\n",
    "    next_month = next_month_datetime.month\n",
    "\n",
    "    # Create a DataFrame for the next month\n",
    "    next_pickup_row_month = Row(pickup_hour=next_month_hour, pickup_day_of_week=next_day_of_week, pickup_month=next_month)\n",
    "    next_pickup_df_month = spark.createDataFrame([next_pickup_row_month])\n",
    "\n",
    "    # Convert columns to feature vector using VectorAssembler\n",
    "    assembler = VectorAssembler(inputCols=[\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"], outputCol=\"features\")\n",
    "\n",
    "    # Convert columns to feature vector using VectorAssembler\n",
    "    next_pickup_df_month = assembler.transform(next_pickup_df_month)\n",
    "\n",
    "    # Make predictions for the next month\n",
    "    predictions_month = lr_model.transform(next_pickup_df_month)\n",
    "    predicted_pickups_month = predictions_month.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "\n",
    "    print(\"Predicted Pickups for Next Month:\", predicted_pickups_month)\n",
    "\n",
    "prediction_month()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
