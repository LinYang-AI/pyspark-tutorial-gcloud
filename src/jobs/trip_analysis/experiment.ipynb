{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/04 20:37:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"NYC_Taxi_Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(\"NYC/*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/04 21:36:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/04 21:36:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYC_Taxi_Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# For convenience, I save the data in the NYC folder which is in the same directory as this notebook\n",
    "parquet_file_path = os.path.join(cwd, \"NYC\", \"*.parquet\")\n",
    "taxi_df = spark.read.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Duration and Distance:\n",
    "Create a new column for trip duration and calculate it using the difference between pickup and dropoff times. Also, calculate the average distance for each record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "taxi_df = taxi_df.withColumn(\"trip_duration\", F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.withColumn(\"avg_distance\", (F.col(\"trip_distance\") / F.col(\"passenger_count\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Time of Day, Day of Week, and Month of Year:\n",
    "- Extract the desired time components from the tpep_pickup_datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "taxi_df = taxi_df.withColumn(\"pickup_day_of_week\", F.dayofweek(\"tpep_pickup_datetime\"))\n",
    "taxi_df = taxi_df.withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group and Aggregate:\n",
    "Group the data by time of day, day of week, and month of year, and calculate the average duration and distance for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = taxi_df.groupBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\").agg(\n",
    "    F.avg(\"trip_duration\").alias(\"avg_duration\"),\n",
    "    F.avg(\"avg_distance\").alias(\"avg_distance\")\n",
    ").orderBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Results:\n",
    "Show the aggregated results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|pickup_hour|pickup_day_of_week|pickup_month|      avg_duration|      avg_distance|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|          0|                 1|           1| 888.9417608770127|3.2521391261294736|\n",
      "|          0|                 1|           2| 726.9726522187823|  2.89541996285979|\n",
      "|          0|                 1|           3| 888.4640534063677|2.8471578638497625|\n",
      "|          0|                 1|           4| 854.6067429406037|2.9782131248083417|\n",
      "|          0|                 1|           5| 883.9497659700705|2.8323654661521616|\n",
      "|          0|                 1|           6| 954.6639178045153| 2.721734596752614|\n",
      "|          0|                 1|           7|1011.6342042755344| 2.835302943232113|\n",
      "|          0|                 1|           8| 997.7019489609131|2.8669667749237213|\n",
      "|          0|                 1|           9|1057.5752172184677|2.5259332031492243|\n",
      "|          0|                 1|          10| 983.6769738118331|2.4983152044322683|\n",
      "|          0|                 1|          11| 935.9346515215258| 2.539192359159652|\n",
      "|          0|                 1|          12| 972.2170118615943| 3.150238748796069|\n",
      "|          0|                 2|           1| 915.7601380500431| 4.946866130558188|\n",
      "|          0|                 2|           2|1105.8451242829829| 4.494771421675113|\n",
      "|          0|                 2|           3|  820.387349953832| 5.216901033850259|\n",
      "|          0|                 2|           4| 960.7860179499291| 5.048313027017737|\n",
      "|          0|                 2|           5| 951.8014098690836| 4.508350586080599|\n",
      "|          0|                 2|           6| 971.4392452830189| 4.867711624592842|\n",
      "|          0|                 2|           7| 990.5213815789474|4.3910269512339335|\n",
      "|          0|                 2|           8|1029.8973904639174| 5.536793713681965|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group and Count Pickup Locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_locations = taxi_df.groupBy(\"PULocationID\").count().orderBy(F.desc(\"count\"))\n",
    "top_pickup_locations = pickup_locations.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoff_locations = taxi_df.groupBy(\"DOLocationID\").count().orderBy(F.desc(\"count\"))\n",
    "top_dropoff_locations = dropoff_locations.limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Pickup Locations:\n",
      "+------------+-------+\n",
      "|PULocationID|  count|\n",
      "+------------+-------+\n",
      "|         237|1553554|\n",
      "|         236|1424614|\n",
      "|         161|1091329|\n",
      "|         132|1025063|\n",
      "|         186|1019650|\n",
      "|         142| 989927|\n",
      "|         170| 967766|\n",
      "|         162| 954917|\n",
      "|         239| 932473|\n",
      "|         141| 909845|\n",
      "|          48| 880679|\n",
      "|         234| 824127|\n",
      "|         163| 798825|\n",
      "|          79| 775824|\n",
      "|         107| 752810|\n",
      "|         238| 742175|\n",
      "|         263| 736487|\n",
      "|          68| 724723|\n",
      "|         230| 721741|\n",
      "|         140| 701603|\n",
      "+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Pickup Locations:\")\n",
    "pickup_locations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Dropoff Locations:\n",
      "+------------+-------+\n",
      "|DOLocationID|  count|\n",
      "+------------+-------+\n",
      "|         236|1434919|\n",
      "|         237|1356518|\n",
      "|         161|1001077|\n",
      "|         170| 920433|\n",
      "|         141| 902052|\n",
      "|         239| 886837|\n",
      "|         142| 854324|\n",
      "|          48| 782803|\n",
      "|         238| 779046|\n",
      "|         162| 772823|\n",
      "|         234| 719605|\n",
      "|         263| 700601|\n",
      "|          68| 692253|\n",
      "|         140| 688235|\n",
      "|         163| 685523|\n",
      "|         229| 677775|\n",
      "|         186| 662011|\n",
      "|         230| 649051|\n",
      "|          79| 638139|\n",
      "|         107| 612836|\n",
      "+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Dropoff Locations:\")\n",
    "dropoff_locations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tip analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the tip percentage for each trip. This involves dividing the tip amount by the total fare amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_analysis_df = taxi_df.withColumn(\"tip_percentage\", F.col(\"tip_amount\") / F.col(\"total_amount\") * 100)\n",
    "tip_analysis_df = tip_analysis_df.withColumn(\"tip_percentage\", F.when(F.col(\"tip_percentage\") <= 100, F.col(\"tip_percentage\")).otherwise(0))  # Handle possible outliers\n",
    "\n",
    "# Group by pickup and dropoff locations and calculate average tip percentage and average distance\n",
    "tip_location_analysis = tip_analysis_df.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "    F.avg(\"tip_percentage\").alias(\"avg_tip_percentage\"),\n",
    "    F.avg(\"trip_distance\").alias(\"avg_distance\")\n",
    ").orderBy(F.desc(\"avg_tip_percentage\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the average tip percentage and total tip amount for each time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_time_analysis = tip_analysis_df.groupBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\").agg(\n",
    "    F.avg(\"tip_percentage\").alias(\"avg_tip_percentage\"),\n",
    "    F.sum(\"tip_amount\").alias(\"total_tip_amount\")\n",
    ").orderBy(\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tip Analysis by Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+-----------------+\n",
      "|PULocationID|DOLocationID|avg_tip_percentage|     avg_distance|\n",
      "+------------+------------+------------------+-----------------+\n",
      "|         187|         251|56.179775280898866|             1.54|\n",
      "|         176|         176|          53.90625|             0.32|\n",
      "|          96|         236| 48.85197850512946|            11.31|\n",
      "|         109|         172|46.948356807511736|             2.09|\n",
      "|         251|         161|46.200737170399776|            19.58|\n",
      "|         120|         151| 43.01075268817204|             0.73|\n",
      "|         118|         214|  41.3564929693962|             4.16|\n",
      "|         172|         214| 39.96670910603205|3.293333333333333|\n",
      "|         208|         114| 38.55192080359299|            8.975|\n",
      "|          34|           1| 38.41764929631039|            15.11|\n",
      "|          82|         253| 38.28483920367534|             2.48|\n",
      "|         112|         214| 37.67972235994051|            16.03|\n",
      "|          96|         177|37.522401433691755|4.699999999999999|\n",
      "|          98|         253|36.231884057971016|              5.0|\n",
      "|          34|         236|36.050057963748074|            8.965|\n",
      "|         214|         172| 35.85657881901123|4.154999999999999|\n",
      "|         118|         172| 34.72222222222222|             3.47|\n",
      "|         175|          28|34.692603978968656|4.966666666666666|\n",
      "|          98|         191| 34.63261668452944|            2.286|\n",
      "|         202|         191| 34.56221198156682|            16.99|\n",
      "+------------+------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Tip Analysis by Location:\")\n",
    "tip_location_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tip Analysis by Time:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|pickup_hour|pickup_day_of_week|pickup_month|avg_tip_percentage|  total_tip_amount|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "|          0|                 1|           1|10.509465545793379| 6285.209999999994|\n",
      "|          0|                 1|           2|10.960206819588734|  8032.64000000002|\n",
      "|          0|                 1|           3|11.105551868685199|12657.859999999993|\n",
      "|          0|                 1|           4|11.314341113424645|18623.729999999974|\n",
      "|          0|                 1|           5|12.117369325323516|36288.720000000074|\n",
      "|          0|                 1|           6| 12.19923946046261| 36665.33000000003|\n",
      "|          0|                 1|           7|11.893981566402744| 34842.08999999987|\n",
      "|          0|                 1|           8|12.147081300758224|47726.230000000294|\n",
      "|          0|                 1|           9|12.178405026405924|43128.000000000204|\n",
      "|          0|                 1|          10|12.615973213402029|64931.850000001265|\n",
      "|          0|                 1|          11|12.530480004739998|52851.220000000736|\n",
      "|          0|                 1|          12|12.381016811103633| 40848.40999999994|\n",
      "|          0|                 2|           1| 9.083634613534988| 2937.700000000002|\n",
      "|          0|                 2|           2| 9.046358567573382| 3625.010000000003|\n",
      "|          0|                 2|           3| 8.912576772587578| 5795.150000000005|\n",
      "|          0|                 2|           4| 9.551659729865403| 6010.509999999994|\n",
      "|          0|                 2|           5|10.373283655908706|          13920.53|\n",
      "|          0|                 2|           6| 10.56761617803041| 16237.30999999995|\n",
      "|          0|                 2|           7|10.498185701824484|17556.769999999986|\n",
      "|          0|                 2|           8|10.404594988408133| 20802.27999999999|\n",
      "+-----------+------------------+------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Tip Analysis by Time:\")\n",
    "tip_time_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Group the data by payment type and calculate statistics such as the average tip percentage, average tip amount, and total tip amount for each payment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_tip_analysis = tip_analysis_df.groupBy(\"payment_type\").agg(\n",
    "    F.avg(\"tip_percentage\").alias(\"avg_tip_percentage\"),\n",
    "    F.avg(\"tip_amount\").alias(\"avg_tip_amount\"),\n",
    "    F.sum(\"tip_amount\").alias(\"total_tip_amount\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payment and Tip Analysis:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=====>                                                   (1 + 8) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+-------------------+\n",
      "|payment_type|  avg_tip_percentage|      avg_tip_amount|   total_tip_amount|\n",
      "+------------+--------------------+--------------------+-------------------+\n",
      "|           0|   8.070807011379319|  2.1700068168216093| 3208778.2300000293|\n",
      "|           5|                 0.0|                 0.0|                0.0|\n",
      "|           1|  15.331362787552713|  3.0755510306721945|6.913956203000465E7|\n",
      "|           3|   0.145542231109957|-0.01167058060330...|-1799.0200000000004|\n",
      "|           2|-0.00155152365292...|4.108590704647675E-4| 2740.4299999999994|\n",
      "|           4| -0.3495420614922692|0.022958282745690756|            2779.72|\n",
      "+------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Payment and Tip Analysis:\")\n",
    "payment_tip_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fare Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the average fare by pickup and dropoff location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_location_analysis = taxi_df.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "    F.avg(\"fare_amount\").alias(\"avg_fare\")\n",
    ").orderBy(F.desc(\"avg_fare\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the average fare for different passenger counts to analyze the correlation between passenger count and fare amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_passenger_analysis = taxi_df.groupBy(\"passenger_count\").agg(\n",
    "    F.avg(\"fare_amount\").alias(\"avg_fare\")\n",
    ").orderBy(\"passenger_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the Pearson correlation coefficient to investigate the correlation between fare amount and trip distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_distance_correlation = taxi_df.select(F.corr(\"fare_amount\", \"trip_distance\").alias(\"correlation\")).collect()[0][\"correlation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fare by Pickup & Drop Location:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------------+\n",
      "|PULocationID|DOLocationID|          avg_fare|\n",
      "+------------+------------+------------------+\n",
      "|         154|          28|            1164.0|\n",
      "|         234|         189| 843.4665424430641|\n",
      "|           1|         247|             420.0|\n",
      "|          83|         136|             378.5|\n",
      "|           5|          74|             306.0|\n",
      "|          54|         265|             275.5|\n",
      "|          29|         264|213.75227272727273|\n",
      "|           2|         265|            200.25|\n",
      "|           6|         265|            192.25|\n",
      "|         123|         265|            177.35|\n",
      "|         235|         115|             170.0|\n",
      "|         221|         265|             160.0|\n",
      "|         253|         208|             160.0|\n",
      "|         112|         109|             155.0|\n",
      "|         204|         265|           152.375|\n",
      "|          44|         138|             151.5|\n",
      "|         118|         265|            151.25|\n",
      "|          55|           1|             150.0|\n",
      "|          10|           1|148.21428571428572|\n",
      "|         221|           1|             148.0|\n",
      "+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Average Fare by Pickup & Drop Location:\")\n",
    "fare_location_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fare by Passenger Count:\n",
      "+---------------+------------------+\n",
      "|passenger_count|          avg_fare|\n",
      "+---------------+------------------+\n",
      "|           null| 25.50271663190539|\n",
      "|            0.0|12.251760226150822|\n",
      "|            1.0|12.709557736571188|\n",
      "|            2.0| 13.77639929394148|\n",
      "|            3.0|13.555663818461737|\n",
      "|            4.0|14.284687986716264|\n",
      "|            5.0|12.666400646383593|\n",
      "|            6.0| 12.75109443706296|\n",
      "|            7.0| 52.91679487179488|\n",
      "|            8.0| 49.14408163265307|\n",
      "|            9.0|             61.35|\n",
      "|           96.0|              11.5|\n",
      "|          112.0|               9.0|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Average Fare by Passenger Count:\")\n",
    "fare_passenger_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between Fare Amount and Trip Distance:\n",
      "Correlation coefficient: 0.0008730862657094112\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation between Fare Amount and Trip Distance:\")\n",
    "print(\"Correlation coefficient:\", fare_distance_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the speed per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_speed_df = taxi_df.withColumn(\"trip_speed\", (F.col(\"trip_distance\") / (F.col(\"trip_duration\") / 3600)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Group by Trip Time Intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_time_speed_analysis = trip_speed_df.groupBy(\"PULocationID\", \"DOLocationID\", \"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\").agg(\n",
    "    F.avg(\"trip_speed\").alias(\"avg_trip_speed\")\n",
    ").orderBy(\"PULocationID\", \"DOLocationID\", \"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "23/08/04 21:47:53 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 36:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----------+------------------+------------+-----------------+\n",
      "|PULocationID|DOLocationID|pickup_hour|pickup_day_of_week|pickup_month|   avg_trip_speed|\n",
      "+------------+------------+-----------+------------------+------------+-----------------+\n",
      "|           1|           1|          0|                 1|           4|              0.0|\n",
      "|           1|           1|          0|                 2|           3|              0.0|\n",
      "|           1|           1|          0|                 2|          11|              0.0|\n",
      "|           1|           1|          0|                 4|           6|              0.0|\n",
      "|           1|           1|          0|                 7|           5|2438.181818181818|\n",
      "|           1|           1|          1|                 1|           6|              0.0|\n",
      "|           1|           1|          1|                 1|          11|              0.0|\n",
      "|           1|           1|          1|                 2|           3|              0.0|\n",
      "|           1|           1|          1|                 2|          10|              0.0|\n",
      "|           1|           1|          1|                 3|           4|              0.0|\n",
      "|           1|           1|          1|                 3|          10|              0.0|\n",
      "|           1|           1|          1|                 4|           6|              0.0|\n",
      "|           1|           1|          1|                 4|          12|              0.0|\n",
      "|           1|           1|          1|                 5|           9|              4.8|\n",
      "|           1|           1|          1|                 7|           4|              0.0|\n",
      "|           1|           1|          2|                 2|           3|              0.0|\n",
      "|           1|           1|          2|                 2|           8|              0.0|\n",
      "|           1|           1|          2|                 2|           9|              0.0|\n",
      "|           1|           1|          2|                 3|           9|              0.0|\n",
      "|           1|           1|          2|                 3|          11|              0.0|\n",
      "+------------+------------+-----------+------------------+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip_time_speed_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create features from the date and time of pickups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "demand_df = taxi_df.withColumn(\"pickup_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "demand_df = demand_df.withColumn(\"pickup_day_of_week\", F.dayofweek(\"tpep_pickup_datetime\"))\n",
    "demand_df = demand_df.withColumn(\"pickup_month\", F.month(\"tpep_pickup_datetime\"))\n",
    "\n",
    "feature_columns = [\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "demand_df = assembler.transform(demand_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regression Model (Linear Regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"input_features\")\n",
    "demand_df = assembler.transform(demand_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data for regression\n",
    "regression_df = demand_df.groupBy(\"pickup_hour\").agg(\n",
    "    F.sum(\"passenger_count\").alias(\"total_pickups\"),\n",
    "    F.first(\"input_features\").alias(\"features\")  # Take any value since they're the same for each group\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/04 22:01:25 WARN Instrumentation: [b9fdd9b6] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/08/04 22:01:26 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/04 22:01:26 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/08/04 22:01:26 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_pickups\")\n",
    "lr_model = lr.fit(regression_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Pickups for Next Hour: 1341885.7375189392\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import hour, dayofweek, month, col\n",
    "from datetime import timedelta\n",
    "\n",
    "# Get the last record in the Parquet data\n",
    "last_record = taxi_df.orderBy(F.desc(\"tpep_pickup_datetime\")).limit(1).collect()[0]\n",
    "\n",
    "# Extract relevant information from the last record\n",
    "last_pickup_datetime = last_record[\"tpep_pickup_datetime\"]\n",
    "next_pickup_datetime = last_pickup_datetime + timedelta(hours=1)\n",
    "\n",
    "# Extract features for the next hour\n",
    "next_hour = next_pickup_datetime.hour\n",
    "next_day_of_week = next_pickup_datetime.weekday()\n",
    "next_month = next_pickup_datetime.month\n",
    "\n",
    "# Create a DataFrame for the next hour\n",
    "next_pickup_row_hour = Row(pickup_hour=next_hour, pickup_day_of_week=next_day_of_week, pickup_month=next_month)\n",
    "next_pickup_df_hour = spark.createDataFrame([next_pickup_row_hour])\n",
    "\n",
    "# Convert columns to feature vector using VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "# Transform the features and make predictions for the next hour\n",
    "next_pickup_df_hour = assembler.transform(next_pickup_df_hour)\n",
    "predictions_hour = lr_model.transform(next_pickup_df_hour)\n",
    "predicted_pickups_hour = predictions_hour.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "\n",
    "print(\"Predicted Pickups for Next Hour:\", predicted_pickups_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Pickups for Next Day: 1461753.3968590284\n"
     ]
    }
   ],
   "source": [
    "# Calculate the next day and month\n",
    "next_day_datetime = last_pickup_datetime + timedelta(days=1)\n",
    "next_day_hour = next_day_datetime.hour\n",
    "next_day_of_week = next_day_datetime.weekday()\n",
    "next_month = next_day_datetime.month\n",
    "\n",
    "# Create a DataFrame for the next day\n",
    "next_pickup_row_day = Row(pickup_hour=next_day_hour, pickup_day_of_week=next_day_of_week, pickup_month=next_month)\n",
    "next_pickup_df_day = spark.createDataFrame([next_pickup_row_day])\n",
    "\n",
    "# Convert columns to feature vector using VectorAssembler\n",
    "next_pickup_df_day = assembler.transform(next_pickup_df_day)\n",
    "\n",
    "# Make predictions for the next day\n",
    "predictions_day = lr_model.transform(next_pickup_df_day)\n",
    "predicted_pickups_day = predictions_day.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "\n",
    "print(\"Predicted Pickups for Next Day:\", predicted_pickups_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Pickups for Next Month: 1198863.0639412608\n"
     ]
    }
   ],
   "source": [
    "# Calculate the next month\n",
    "next_month_datetime = last_pickup_datetime + timedelta(days=30)  # Assuming 30 days in a month\n",
    "next_month_hour = next_month_datetime.hour\n",
    "next_day_of_week = next_month_datetime.weekday()\n",
    "next_month = next_month_datetime.month\n",
    "\n",
    "# Create a DataFrame for the next month\n",
    "next_pickup_row_month = Row(pickup_hour=next_month_hour, pickup_day_of_week=next_day_of_week, pickup_month=next_month)\n",
    "next_pickup_df_month = spark.createDataFrame([next_pickup_row_month])\n",
    "\n",
    "# Convert columns to feature vector using VectorAssembler\n",
    "next_pickup_df_month = assembler.transform(next_pickup_df_month)\n",
    "\n",
    "# Make predictions for the next month\n",
    "predictions_month = lr_model.transform(next_pickup_df_month)\n",
    "predicted_pickups_month = predictions_month.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "\n",
    "print(\"Predicted Pickups for Next Month:\", predicted_pickups_month)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
